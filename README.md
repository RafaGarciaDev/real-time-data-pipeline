# ğŸš€ Pipeline de Dados em Tempo Real

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Kafka](https://img.shields.io/badge/Apache%20Kafka-3.0-black.svg)
![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

Um pipeline de dados em tempo real completo para processamento e anÃ¡lise de streaming de dados, utilizando Apache Kafka, Python, PostgreSQL e visualizaÃ§Ã£o em tempo real.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Tecnologias](#tecnologias)
- [Funcionalidades](#funcionalidades)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso](#uso)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Monitoramento](#monitoramento)
- [Contribuindo](#contribuindo)
- [LicenÃ§a](#licenÃ§a)

## ğŸ¯ VisÃ£o Geral

Este projeto demonstra a implementaÃ§Ã£o de um pipeline de dados em tempo real que:

- Gera dados simulados de eventos de e-commerce
- Processa streams de dados usando Apache Kafka
- Armazena dados processados em PostgreSQL
- Visualiza mÃ©tricas em tempo real via dashboard web
- Monitora a saÃºde do pipeline

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Producer  â”‚ â†’ Gera eventos simulados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Kafka   â”‚ â†’ Message Broker
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Consumer  â”‚ â†’ Processa eventos
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚ â†’ Armazena dados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚ â†’ Visualiza mÃ©tricas
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tecnologias

- **Apache Kafka**: Message broker para streaming de dados
- **Python 3.9+**: Linguagem principal
- **PostgreSQL**: Banco de dados relacional
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o
- **Flask**: Framework web para dashboard
- **Plotly**: VisualizaÃ§Ã£o de dados
- **Kafka-Python**: Cliente Kafka para Python

## âœ¨ Funcionalidades

- âœ… GeraÃ§Ã£o de dados simulados de e-commerce em tempo real
- âœ… Processamento de streams com Apache Kafka
- âœ… TransformaÃ§Ã£o e enriquecimento de dados
- âœ… PersistÃªncia em banco de dados relacional
- âœ… Dashboard web interativo com mÃ©tricas em tempo real
- âœ… Monitoramento de health do pipeline
- âœ… Logs estruturados
- âœ… Tratamento de erros e retry logic
- âœ… ContainerizaÃ§Ã£o completa com Docker

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.9+ (para desenvolvimento local)
- 4GB de RAM disponÃ­vel

### Passos

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/real-time-data-pipeline.git
cd real-time-data-pipeline
```

2. Inicie os serviÃ§os com Docker Compose:
```bash
docker-compose up -d
```

3. Aguarde todos os serviÃ§os iniciarem (cerca de 30 segundos):
```bash
docker-compose ps
```

4. Acesse o dashboard:
```
http://localhost:5000
```

## ğŸ’» Uso

### Iniciar o Pipeline

```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Verificar logs
docker-compose logs -f

# Verificar logs de um serviÃ§o especÃ­fico
docker-compose logs -f producer
docker-compose logs -f consumer
```

### Parar o Pipeline

```bash
docker-compose down
```

### Limpar dados e recomeÃ§ar

```bash
docker-compose down -v
docker-compose up -d
```

### Desenvolvimento Local

```bash
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# Executar producer
python src/producer.py

# Executar consumer
python src/consumer.py

# Executar dashboard
python src/dashboard.py
```

## ğŸ“ Estrutura do Projeto

```
real-time-data-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer.py          # Gerador de dados
â”‚   â”œâ”€â”€ consumer.py          # Consumidor Kafka
â”‚   â”œâ”€â”€ dashboard.py         # Dashboard web
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ utils.py             # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.producer
â”‚   â”œâ”€â”€ Dockerfile.consumer
â”‚   â””â”€â”€ Dockerfile.dashboard
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql             # Schema do banco
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ dashboard.js
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Template do dashboard
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_producer.py
â”‚   â”œâ”€â”€ test_consumer.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ docker-compose.yml       # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ .env.example            # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ“Š Monitoramento

### MÃ©tricas DisponÃ­veis

O dashboard exibe as seguintes mÃ©tricas em tempo real:

- **Total de Eventos**: NÃºmero total de eventos processados
- **Eventos por Segundo**: Taxa de throughput
- **Receita Total**: Valor total de transaÃ§Ãµes
- **Produtos Mais Vendidos**: Top 10 produtos
- **DistribuiÃ§Ã£o por Categoria**: Vendas por categoria
- **Timeline de Eventos**: GrÃ¡fico temporal

### Health Checks

Verificar saÃºde dos serviÃ§os:

```bash
# Kafka
curl http://localhost:9092

# PostgreSQL
docker-compose exec postgres psql -U admin -d pipeline_db -c "SELECT 1;"

# Dashboard
curl http://localhost:5000/health
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Executar com cobertura
pytest --cov=src tests/

# Executar testes especÃ­ficos
pytest tests/test_producer.py
```

## ğŸ”§ ConfiguraÃ§Ã£o

Copie o arquivo `.env.example` para `.env` e ajuste as variÃ¡veis:

```env
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=ecommerce-events
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=pipeline_db
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin123
```

## ğŸ“ˆ Melhorias Futuras

- [ ] Implementar Apache Flink para processamento complexo
- [ ] Adicionar Redis para cache
- [ ] Implementar alertas via email/Slack
- [ ] Adicionar autenticaÃ§Ã£o ao dashboard
- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar testes de carga
- [ ] Implementar CI/CD com GitHub Actions
- [ ] Adicionar Grafana para mÃ©tricas avanÃ§adas

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a um Fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ‘¤ Autor

Seu Nome
- GitHub: [@seu-usuario](https://github.com/seu-usuario)
- LinkedIn: [seu-perfil](https://linkedin.com/in/seu-perfil)

## ğŸ™ Agradecimentos

- Apache Kafka Community
- Docker Community
- Comunidade Python

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela!
